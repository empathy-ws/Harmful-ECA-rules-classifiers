{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"UgzD9F-vWatj"},"outputs":[],"source":["from numpy import array\n","from keras.preprocessing.text import one_hot\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers.core import Activation, Dropout, Dense, SpatialDropout1D\n","from keras.layers import Flatten, LSTM\n","from keras.layers import GlobalMaxPooling1D\n","from sklearn.model_selection import train_test_split, KFold, cross_val_score\n","from keras.models import Model\n","from keras.layers.embeddings import Embedding\n","from keras.preprocessing.text import Tokenizer\n","from keras.layers import Input\n","from keras.layers.merge import Concatenate\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","\n","from sklearn import preprocessing\n","\n","import pandas as pd\n","import numpy as np\n","import re\n","\n","from numpy import array\n","from numpy import asarray\n","from numpy import zeros\n","\n","import nltk\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","\n","from keras.utils.np_utils import to_categorical\n","\n","from keras.utils.vis_utils import plot_model\n","\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EafaVnEINxC0"},"outputs":[],"source":["STOPWORDS = set(stopwords.words('english'))\n","REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m7Gzwk8Zbarz"},"outputs":[],"source":["def preprocess_text(sen):\n","\n","    stemmer = WordNetLemmatizer()\n","\n","   # Remove all the special characters\n","    document = re.sub(r'\\W', ' ', str(sen))\n","    \n","    # remove all single characters\n","    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n","    \n","    # Remove single characters from the start\n","    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n","\n","    document = REPLACE_BY_SPACE_RE.sub(' ', document)\n","    \n","    # Substituting multiple spaces with single space\n","    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n","    \n","    # Converting to Lowercase\n","    document = document.lower()\n","  \n","    document = document.split()\n","\n","    document = ' '.join(word for word in document if word not in STOPWORDS) # remove stopwors from text\n","\n","    # Lemmatization\n","  \n","    document = document.split()\n","\n","    document = [stemmer.lemmatize(word) for word in document]\n","    document = ' '.join(document) \n","    \n","    return document"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s8JS-I4kCau3"},"outputs":[],"source":["def getModel(maxlen, embedding_matrix):\n","  input_1 = Input(shape=(maxlen,))\n","  input_2 = Input(shape=(4,)) \n","\n","  embedding_layer = Embedding(vocab_size, 50, weights=[embedding_matrix], trainable=False)(input_1)\n","  LSTM_Layer_1 = LSTM(70, activation='tanh')(embedding_layer)\n","\n","  dense_layer_1 = Dense(50, activation='tanh')(input_2)\n","  dense_layer_2 = Dense(20, activation='tanh')(dense_layer_1)\n","\n","  concat_layer = Concatenate()([LSTM_Layer_1, dense_layer_2])\n","  dense_layer_3 = Dense(10, activation='tanh')(concat_layer)\n","  output = Dense(4, activation='softmax')(dense_layer_3)\n","  \n","  return Model(inputs=[input_1, input_2], outputs=output)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"khx1iiQKCbN7"},"outputs":[],"source":["def weighted_categorical_crossentropy(weights):\n","    \"\"\"\n","    A weighted version of keras.objectives.categorical_crossentropy\n","\n","    Variables:\n","        weights: numpy array of shape (C,) where C is the number of classes\n","\n","    Usage:\n","        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n","        loss = weighted_categorical_crossentropy(weights)\n","        model.compile(loss=loss,optimizer='adam')\n","    \"\"\"\n","\n","    weights = K.variable(weights)\n","\n","    def loss(y_true, y_pred):\n","        # scale predictions so that the class probas of each sample sum to 1\n","        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n","        # clip to prevent NaN's and Inf's\n","        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n","        # calc\n","        loss = y_true * K.log(y_pred) * weights\n","        loss = -K.sum(loss, -1)\n","        return loss\n","\n","    return loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NJbC88QibeLr"},"outputs":[],"source":["train_path = 'Training_set.csv'\n","\n","col_names = ['triggerTitle','triggerChannelTitle','actionChannelTitle','actionTitle','title', 'desc', 'target']\n","train_final = pd.read_csv(train_path,skiprows=1,sep=',',names=col_names,encoding = \"ISO-8859-1\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_5pjlvnp21zh"},"outputs":[],"source":["test_path = 'Test_set.csv'\n","\n","col_names = ['triggerTitle','triggerChannelTitle','actionChannelTitle','actionTitle','title', 'desc', 'target']\n","test_final = pd.read_csv(test_path,skiprows=1,sep=';',names=col_names,encoding = \"ISO-8859-1\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6qxDnwtR3FyC"},"outputs":[],"source":["train_df_extract = train_final[[\"title\", \"desc\"]]\n","\n","title_desc_concat = []\n","\n","for i in range(0,len(train_df_extract)):\n","  sentence = str(train_df_extract.iloc[i][0]) + \". \" + str(train_df_extract.iloc[i][1] + \".\")\n","  title_desc_concat.append(sentence)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SXaDPbZ7btdD"},"outputs":[],"source":["del train_final['title']\n","del train_final['desc']\n","\n","target_column = train_final[[\"target\"]]\n","del train_final['target']\n","\n","train_final['sentence'] = title_desc_concat\n","train_final['target'] = target_column\n","\n","# label_encoder object knows how to understand word labels.\n","label_encoder = preprocessing.LabelEncoder()\n","\n","# Encode labels in column 'species'.\n","train_final['triggerTitle'] = label_encoder.fit_transform(train_final['triggerTitle'])\n","train_final['triggerChannelTitle'] = label_encoder.fit_transform(train_final['triggerChannelTitle'])\n","train_final['actionChannelTitle'] = label_encoder.fit_transform(train_final['actionChannelTitle'])\n","train_final['actionTitle'] = label_encoder.fit_transform(train_final['actionTitle'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-KhN_SpHbxWR"},"outputs":[],"source":["X_train = train_final.drop('target', axis=1)\n","\n","y_train = train_final['target']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ODg5qXDX3YHa"},"outputs":[],"source":["test_df_extract = test_final[[\"title\", \"desc\"]]\n","\n","title_desc_concat = []\n","\n","for i in range(0,len(test_df_extract)):\n","  sentence = str(test_df_extract.iloc[i][0]) + \". \" + str(test_df_extract.iloc[i][1])\n","  title_desc_concat.append(sentence)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CBFpkx493jFw"},"outputs":[],"source":["from sklearn import preprocessing\n","\n","del test_final['title']\n","del test_final['desc']\n","\n","target_column = test_final[[\"target\"]]\n","del test_final['target']\n","\n","test_final['sentence'] = title_desc_concat\n","test_final['target'] = target_column\n","\n","# label_encoder object knows how to understand word labels.\n","label_encoder = preprocessing.LabelEncoder()\n","\n","# Encode labels in column 'species'.\n","test_final['triggerTitle'] = label_encoder.fit_transform(test_final['triggerTitle'])\n","test_final['triggerChannelTitle'] = label_encoder.fit_transform(test_final['triggerChannelTitle'])\n","test_final['actionChannelTitle'] = label_encoder.fit_transform(test_final['actionChannelTitle'])\n","test_final['actionTitle'] = label_encoder.fit_transform(test_final['actionTitle'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ffSak51l3jpQ"},"outputs":[],"source":["X_test = test_final.drop('target', axis=1)\n","\n","y_test = test_final['target']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lxGoT9-kc_pL"},"outputs":[],"source":["y_train = to_categorical(y_train)\n","y_test = to_categorical(y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DXkUFjWKd1yw"},"outputs":[],"source":["X1_train = []\n","sentences = list(X_train[\"sentence\"])\n","for sen in sentences:\n","    X1_train.append(preprocess_text(sen))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FwmfJTmkeQFH"},"outputs":[],"source":["X1_test = []\n","sentences = list(X_test[\"sentence\"])\n","for sen in sentences:\n","    X1_test.append(preprocess_text(sen))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dQkXSuoxlJ07"},"outputs":[],"source":["tokenizer = Tokenizer(lower=True, num_words=None)\n","tokenizer.fit_on_texts(X1_train)\n","\n","X1_train = tokenizer.texts_to_sequences(X1_train)\n","X1_test = tokenizer.texts_to_sequences(X1_test)\n","\n","vocab_size = len(tokenizer.word_index) + 1\n","\n","maxlen = 50\n","\n","X1_train = pad_sequences(X1_train, padding='post', maxlen=maxlen)\n","X1_test = pad_sequences(X1_test, padding='post', maxlen=maxlen)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PYKaJH1wlW6a"},"outputs":[],"source":["from numpy import array\n","from numpy import asarray\n","from numpy import zeros\n","\n","embeddings_dictionary = dict()\n","\n","glove_file = open('Glove/glove.6B.50d.txt', encoding=\"utf8\")\n","\n","for line in glove_file:\n","    records = line.split()\n","    word = records[0]\n","    vector_dimensions = asarray(records[1:], dtype='float32')\n","    embeddings_dictionary[word] = vector_dimensions\n","\n","glove_file.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MagXuDRHleiQ"},"outputs":[],"source":["embedding_matrix = zeros((vocab_size, 50))\n","for word, index in tokenizer.word_index.items():\n","    embedding_vector = embeddings_dictionary.get(word)\n","    if embedding_vector is not None:\n","        embedding_matrix[index] = embedding_vector"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xvJ1L6_LlnQU"},"outputs":[],"source":["X2_train = X_train[['triggerTitle', 'triggerChannelTitle', 'actionChannelTitle','actionTitle']].values\n","X2_test = X_test[['triggerTitle', 'triggerChannelTitle', 'actionChannelTitle','actionTitle']].values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KHhYnfM7DGJN"},"outputs":[],"source":["# STRATIFIES K-FOLD CROSS VALIDATION { 4-fold }\n","\n","splits = 4\n","\n","# Create StratifiedKFold object.\n","skf = StratifiedKFold(n_splits=splits, shuffle=True, random_state=1)\n","\n","class_weights_list = []\n","\n","for train_index, test_index in skf.split(X_train, y_train):\n","    count = count + 1\n","    x_train1_fold, x_test1_fold = X1_train.iloc[train_index], X1_train.iloc[test_index]\n","    x_train2_fld, x_test2_fold = X2_train.iloc[train_index], X2_train.iloc[test_index]\n","    y_train_fold, y_test_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n","\n","    #compute the class weights\n","    class_weights = compute_class_weight('balanced', np.unique(y_train_fold),y = np.ravel(y_train_fold))\n","\n","    class_weights_list.append(class_weights)\n","\n","    ncce = weighted_categorical_crossentropy(weights=np.array(class_weights))\n","\n","    model = getModel(maxlen, embedding_matrix)\n","\n","    model.compile(loss=ncce, optimizer='adam', metrics=['acc'])\n","\n","    model.fit(x=[x_train1_fold, x_train2_fld], y=y_train_fold, batch_size=10, epochs=35, verbose=1)\n","\n","    score = model.evaluate(x=[x_test1_fold, x_test2_fold], y=y_test_fold, verbose=0)\n","\n","    print(\"Accuracy Validation: %.2f%%\" % (score[1]*100))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6DmDQCiMEfy8"},"outputs":[],"source":["ncce = weighted_categorical_crossentropy(weights=np.array(class_weights_list[best_class_weight]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eLEBN0YiEmGf"},"outputs":[],"source":["model = getModel(maxlen, embedding_matrix)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z8SJJfx1nu5n"},"outputs":[],"source":["model.compile(loss=ncce, optimizer='adam', metrics=['acc'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wHbNKNYpoGM1"},"outputs":[],"source":["history = model.fit(x=[X1_train, X2_train], y=y_train, batch_size=10, epochs=35, verbose=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L7lVsOBioXif"},"outputs":[],"source":["score = model.evaluate(x=[X1_test, X2_test], y=y_test, verbose=0)\n","\n","print(\"Accuracy Test: %.2f%%\" % (score[1]*100))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bmdBEDK2_Ufh"},"outputs":[],"source":["from sklearn.metrics import classification_report\n","from sklearn.metrics import confusion_matrix\n","\n","y_pred = model.predict(x=[X1_test, X2_test])\n","\n","y_pred_clean = np.zeros_like(y_pred)\n","for idx, i in enumerate(np.argmax(y_pred,axis=1)):\n","    y_pred_clean[idx][i] = 1\n","\n","print(classification_report(y_test, y_pred_clean))"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"LSTM_AllFeature.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
