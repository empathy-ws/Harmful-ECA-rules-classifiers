{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"-PWsQVkaqXb_"},"outputs":[],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a_g0nULXqd3z"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","import transformers\n","from transformers import AutoModel, BertTokenizerFast\n","import nltk\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","import re\n","\n","import tensorflow as tf\n","\n","device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","  raise SystemError('GPU device not found')\n","print('Found GPU at: {}'.format(device_name))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i7qFfNMpqfbf"},"outputs":[],"source":["import torch\n","\n","# If there's a GPU available...\n","if torch.cuda.is_available():    \n","\n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GWzvxBt9OtEJ"},"outputs":[],"source":["def preprocess_text(sen):\n","\n","    stemmer = WordNetLemmatizer()\n","\n","   # Remove all the special characters\n","    document = re.sub(r'\\W', ' ', str(sen))\n","    \n","    # remove all single characters\n","    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n","    \n","    # Remove single characters from the start\n","    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n","\n","    document = REPLACE_BY_SPACE_RE.sub(' ', document)\n","    \n","    # Substituting multiple spaces with single space\n","    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n","    \n","    # Converting to Lowercase\n","    document = document.lower()\n","  \n","    document = document.split()\n","\n","    document = ' '.join(word for word in document if word not in STOPWORDS) # remove stopwors from text\n","\n","    # Lemmatization\n","  \n","    document = document.split()\n","\n","    document = [stemmer.lemmatize(word) for word in document]\n","    document = ' '.join(document) \n","    \n","    return document"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wYxyzzvlSyhu"},"outputs":[],"source":["# Function to calculate the accuracy of our predictions vs labels\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"205Dk_XL9aHV"},"outputs":[],"source":["import time\n","import datetime\n","\n","def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HpN9CF2iO3Vu"},"outputs":[],"source":["from transformers import BertTokenizer\n","\n","# Load the BERT tokenizer.\n","print('Loading BERT tokenizer...')\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VJ4_13npqikn"},"outputs":[],"source":["train_path = 'Training_set.csv'\n","\n","col_names = ['triggerTitle','triggerChannelTitle','actionChannelTitle','actionTitle','title', 'desc', 'target']\n","train = pd.read_csv(train_path,skiprows=1,sep=';',names=col_names,encoding = \"ISO-8859-1\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nT5MQkQKO6r6"},"outputs":[],"source":["#Pre-processing\n","\n","sentences = []\n","for i in range(0,len(train)):\n","  sentences.append(preprocess_text(train.iloc[i][4] + \". \" + train.iloc[i][5] + \".\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uby7k2Z-7gLp"},"outputs":[],"source":["y = pd.DataFrame({'target':train_final[\"target\"]})\n","x = pd.DataFrame({'sentence':sentences})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eIkRV3B0ZCUy"},"outputs":[],"source":["# STRATIFIES K-FOLD CROSS VALIDATION { 4-fold }\n","   \n","# Import Required Modules.\n","from statistics import mean, stdev\n","from transformers import BertForSequenceClassification, AdamW, BertConfig\n","from sklearn.model_selection import StratifiedKFold\n","from torch.utils.data import TensorDataset\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","from transformers import get_linear_schedule_with_warmup\n","import random\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.utils.class_weight import compute_class_weight\n","% matplotlib inline\n","\n","splits = 4\n","\n","# Create StratifiedKFold object.\n","skf = StratifiedKFold(n_splits=splits, shuffle=True, random_state=1)\n","\n","class_weights_list = []\n","\n","for train_index, test_index in skf.split(x, y):\n","    x_train_fold, x_test_fold = x.iloc[train_index], x.iloc[test_index]\n","    y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]\n","\n","    #compute the class weights\n","    class_weights = compute_class_weight('balanced', np.unique(y_train_fold),y = np.ravel(y_train_fold))\n","    class_weights_list.append(class_weights)\n","\n","    # converting list of class weights to a tensor\n","    weights= torch.tensor(class_weights,dtype=torch.float)\n","\n","    # push to GPU\n","    weights = weights.to(device)\n","\n","    # define the loss function\n","    cross_entropy  = nn.CrossEntropyLoss(weight=weights) \n","\n","    # Load BertForSequenceClassification, the pretrained BERT model with a single \n","    # linear classification layer on top. \n","    model = BertForSequenceClassification.from_pretrained(\n","        \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n","        num_labels = 4, # The number of output labels--4 for multi-class classification. \n","        output_attentions = False, # Whether the model returns attentions weights.\n","        output_hidden_states = False, # Whether the model returns all hidden-states.\n","    )\n","\n","    for param in model.bert.parameters():\n","        param.requires_grad = False\n","\n","    # Tell pytorch to run this model on the GPU.\n","    model.cuda()\n","\n","    # Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n","    # I believe the 'W' stands for 'Weight Decay fix\"\n","    optimizer = AdamW(model.parameters(),\n","                      lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n","                      eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n","                    )\n","\n","    print('======== Split {:} / {:} ========'.format(count, splits))\n","\n","    #------------------------------------------------------\n","\n","    # Get the lists of sentences and their labels.\n","    train_sentences = x_train_fold.sentence.values\n","    train_labels = y_train_fold.target.values \n","    \n","    # Tokenize all of the sentences and map the tokens to thier word IDs.\n","    train_input_ids = []\n","    train_attention_masks = []\n","\n","        # For every sentence...\n","    for sent in train_sentences:\n","        # `encode_plus` will:\n","        #   (1) Tokenize the sentence.\n","        #   (2) Prepend the `[CLS]` token to the start.\n","        #   (3) Append the `[SEP]` token to the end.\n","        #   (4) Map tokens to their IDs.\n","        #   (5) Pad or truncate the sentence to `max_length`\n","        #   (6) Create attention masks for [PAD] tokens.\n","        encoded_dict = tokenizer.encode_plus(\n","                            sent,                      # Sentence to encode.\n","                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                            max_length = 50,           # Pad & truncate all sentences.\n","                            pad_to_max_length = True,\n","                            return_attention_mask = True,   # Construct attn. masks.\n","                            return_tensors = 'pt',     # Return pytorch tensors.\n","                      )\n","        \n","        # Add the encoded sentence to the list.    \n","        train_input_ids.append(encoded_dict['input_ids'])\n","        \n","        # And its attention mask (simply differentiates padding from non-padding).\n","        train_attention_masks.append(encoded_dict['attention_mask'])\n","\n","    # Convert the lists into tensors.\n","    train_input_ids = torch.cat(train_input_ids, dim=0)\n","    train_attention_masks = torch.cat(train_attention_masks, dim=0)\n","    train_labels = torch.tensor(train_labels)\n","\n","    #---------------------------------------------------\n","\n","    # Get the lists of sentences and their labels.\n","    test_sentences = x_test_fold.sentence.values\n","    test_labels = y_test_fold.target.values\n","\n","    # Tokenize all of the sentences and map the tokens to thier word IDs.\n","    test_input_ids = []\n","    test_attention_masks = []\n","\n","        # For every sentence...\n","    for sent in test_sentences:\n","        # `encode_plus` will:\n","        #   (1) Tokenize the sentence.\n","        #   (2) Prepend the `[CLS]` token to the start.\n","        #   (3) Append the `[SEP]` token to the end.\n","        #   (4) Map tokens to their IDs.\n","        #   (5) Pad or truncate the sentence to `max_length`\n","        #   (6) Create attention masks for [PAD] tokens.\n","        encoded_dict = tokenizer.encode_plus(\n","                            sent,                      # Sentence to encode.\n","                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                            max_length = 50,           # Pad & truncate all sentences.\n","                            pad_to_max_length = True,\n","                            return_attention_mask = True,   # Construct attn. masks.\n","                            return_tensors = 'pt',     # Return pytorch tensors.\n","                      )\n","        \n","        # Add the encoded sentence to the list.    \n","        test_input_ids.append(encoded_dict['input_ids'])\n","        \n","        # And its attention mask (simply differentiates padding from non-padding).\n","        test_attention_masks.append(encoded_dict['attention_mask'])\n","\n","    # Convert the lists into tensors.\n","    test_input_ids = torch.cat(test_input_ids, dim=0)\n","    test_attention_masks = torch.cat(test_attention_masks, dim=0)\n","    test_labels = torch.tensor(test_labels)\n","\n","    #---------------------------------------------------\n","\n","    # Combine the training inputs into a TensorDataset.\n","    train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n","\n","    # Combine the test inputs into a TensorDataset. \n","    val_dataset = TensorDataset(test_input_ids, test_attention_masks, test_labels)\n","\n","    #---------------------------------------------------\n","\n","    # The DataLoader needs to know our batch size for training, so we specify it \n","    # here. For fine-tuning BERT on a specific task, the authors recommend a batch \n","    # size of 16 or 32.\n","    batch_size = 32\n","\n","    # Create the DataLoaders for our training and validation sets.\n","    # We'll take training samples in random order. \n","    train_dataloader = DataLoader(\n","                train_dataset,  # The training samples.\n","                sampler = RandomSampler(train_dataset), # Select batches randomly\n","                batch_size = batch_size # Trains with this batch size.\n","            )\n","\n","    # For validation the order doesn't matter, so we'll just read them sequentially.\n","    validation_dataloader = DataLoader(\n","                val_dataset, # The validation samples.\n","                sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n","                batch_size = batch_size # Evaluate with this batch size.\n","            )\n","    \n","    #---------------------------------------------------\n","\n","    # Number of training epochs. The BERT authors recommend between 2 and 4. \n","    # We chose to run for 4, but we'll see later that this may be over-fitting the\n","    # training data.\n","    epochs = 2\n","\n","    # Total number of training steps is [number of batches] x [number of epochs]. \n","    # (Note that this is not the same as the number of training samples).\n","    total_steps = len(train_dataloader) * epochs\n","\n","    # Create the learning rate scheduler.\n","    scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                                num_warmup_steps = 0, # Default value in run_glue.py\n","                                                num_training_steps = total_steps)\n","    \n","    #---------------------------------------------------\n","    \n","    # Set the seed value all over the place to make this reproducible.\n","    seed_val = 42\n","    train_loss_set = []\n","\n","    y_pred = []\n","    y_label = []\n","\n","    random.seed(seed_val)\n","    np.random.seed(seed_val)\n","    torch.manual_seed(seed_val)\n","    torch.cuda.manual_seed_all(seed_val)\n","\n","    # We'll store a number of quantities such as training and validation loss, \n","    # validation accuracy, and timings.\n","    training_stats = []\n","\n","    # Measure the total training time for the whole run.\n","    total_t0 = time.time()\n","\n","    # For each epoch...\n","    for epoch_i in range(0, epochs):\n","        \n","        # ========================================\n","        #               Training\n","        # ========================================\n","        \n","        # Perform one full pass over the training set.\n","\n","        print(\"\")\n","        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","        print('Training...')\n","\n","        # Measure how long the training epoch takes.\n","        t0 = time.time()\n","\n","        # Reset the total loss for this epoch.\n","        total_train_loss = 0\n","\n","        # Put the model into training mode. Don't be mislead--the call to \n","        # `train` just changes the *mode*, it doesn't *perform* the training.\n","        # `dropout` and `batchnorm` layers behave differently during training\n","        # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n","        model.train()\n","\n","        # For each batch of training data...\n","        for step, batch in enumerate(train_dataloader):\n","\n","            # Progress update every 40 batches.\n","            if step % 40 == 0 and not step == 0:\n","                # Calculate elapsed time in minutes.\n","                elapsed = format_time(time.time() - t0)\n","                \n","                # Report progress.\n","                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","            # Unpack this training batch from our dataloader. \n","            #\n","            # As we unpack the batch, we'll also copy each tensor to the GPU using the \n","            # `to` method.\n","            #\n","            # `batch` contains three pytorch tensors:\n","            #   [0]: input ids \n","            #   [1]: attention masks\n","            #   [2]: labels \n","            b_input_ids = batch[0].to(device)\n","            b_input_mask = batch[1].to(device)\n","            b_labels = batch[2].to(device)\n","\n","            # Always clear any previously calculated gradients before performing a\n","            # backward pass. PyTorch doesn't do this automatically because \n","            # accumulating the gradients is \"convenient while training RNNs\". \n","            # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n","            optimizer.zero_grad()        \n","\n","            # Perform a forward pass (evaluate the model on this training batch).\n","            # The documentation for this `model` function is here: \n","            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","            # It returns different numbers of parameters depending on what arguments\n","            # arge given and what flags are set. For our useage here, it returns\n","            # the loss (because we provided labels) and the \"logits\"--the model\n","            # outputs prior to activation.\n","            outputs = model(b_input_ids, \n","                                token_type_ids=None, \n","                                attention_mask=b_input_mask, \n","                                labels=b_labels)\n","            \n","            loss = cross_entropy(outputs.logits, b_labels)\n","\n","            train_loss_set.append(loss.item()) \n","\n","            # Accumulate the training loss over all of the batches so that we can\n","            # calculate the average loss at the end. `loss` is a Tensor containing a\n","            # single value; the `.item()` function just returns the Python value \n","            # from the tensor.\n","\n","            total_train_loss += loss.item()\n","\n","            # Perform a backward pass to calculate the gradients.\n","            loss.backward()\n","\n","            # Clip the norm of the gradients to 1.0.\n","            # This is to help prevent the \"exploding gradients\" problem.\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","            # Update parameters and take a step using the computed gradient.\n","            # The optimizer dictates the \"update rule\"--how the parameters are\n","            # modified based on their gradients, the learning rate, etc.\n","            optimizer.step()\n","\n","            # Update the learning rate.\n","            scheduler.step()\n","\n","        # Calculate the average loss over all of the batches.\n","        avg_train_loss = total_train_loss / len(train_dataloader)            \n","        \n","        # Measure how long this epoch took.\n","        training_time = format_time(time.time() - t0)\n","\n","        print(\"\")\n","        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","        print(\"  Training epcoh took: {:}\".format(training_time))\n","\n","        # ========================================\n","        #               Validation\n","        # ========================================\n","        # After the completion of each training epoch, measure our performance on\n","        # our validation set.\n","\n","        print(\"\")\n","        print(\"Running Validation...\")\n","\n","        t0 = time.time()\n","\n","        # Put the model in evaluation mode--the dropout layers behave differently\n","        # during evaluation.\n","        model.eval()\n","\n","        # Tracking variables \n","        total_eval_accuracy = 0\n","        total_eval_loss = 0\n","        nb_eval_steps = 0\n","\n","        # Evaluate data for one epoch\n","        for batch in validation_dataloader:\n","            \n","            # Unpack this training batch from our dataloader. \n","            #\n","            # As we unpack the batch, we'll also copy each tensor to the GPU using \n","            # the `to` method.\n","            #\n","            # `batch` contains three pytorch tensors:\n","            #   [0]: input ids \n","            #   [1]: attention masks\n","            #   [2]: labels \n","            b_input_ids = batch[0].to(device)\n","            b_input_mask = batch[1].to(device)\n","            b_labels = batch[2].to(device)\n","            \n","            # Tell pytorch not to bother with constructing the compute graph during\n","            # the forward pass, since this is only needed for backprop (training).\n","            with torch.no_grad():        \n","\n","                # Forward pass, calculate logit predictions.\n","                # token_type_ids is the same as the \"segment ids\", which \n","                # differentiates sentence 1 and 2 in 2-sentence tasks.\n","                # The documentation for this `model` function is here: \n","                # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","                # Get the \"logits\" output by the model. The \"logits\" are the output\n","                # values prior to applying an activation function like the softmax.\n","                outputs = model(b_input_ids, \n","                                      token_type_ids=None, \n","                                      attention_mask=b_input_mask,\n","                                      labels=b_labels)\n","                \n","            loss = cross_entropy(outputs.logits, b_labels)\n","            # Accumulate the validation loss.\n","            total_eval_loss += loss.item()\n","\n","            # Move logits and labels to CPU\n","            logits = outputs.logits\n","            logits = logits.detach().cpu().numpy()\n","            label_ids = b_labels.to('cpu').numpy()\n","\n","            # Calculate the accuracy for this batch of test sentences, and\n","            # accumulate it over all batches.\n","            total_eval_accuracy += flat_accuracy(logits, label_ids)\n","\n","        # Report the final accuracy for this validation run.\n","        avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n","        print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n","\n","        # Calculate the average loss over all of the batches.\n","        avg_val_loss = total_eval_loss / len(validation_dataloader)\n","        \n","        # Measure how long the validation run took.\n","        validation_time = format_time(time.time() - t0)\n","        \n","        print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n","        print(\"  Validation took: {:}\".format(validation_time))\n","\n","        # Record all statistics from this epoch.\n","        training_stats.append(\n","            {\n","                'epoch': epoch_i + 1,\n","                'Training Loss': avg_train_loss,\n","                'Valid. Loss': avg_val_loss,\n","                'Valid. Accur.': avg_val_accuracy,\n","                'Training Time': training_time,\n","                'Validation Time': validation_time\n","            }\n","        )\n","\n","    print(\"\")\n","    print(\"Training complete!\")\n","    lst_accu_stratified.append(avg_val_accuracy)\n","\n","    #---------------------------------------------------\n","\n","    # Display floats with two decimal places.\n","    pd.set_option('precision', 2)\n","\n","    # Create a DataFrame from our training statistics.\n","    df_stats = pd.DataFrame(data=training_stats)\n","\n","    # Use the 'epoch' as the row index.\n","    df_stats = df_stats.set_index('epoch')\n","\n","    # Display the table.\n","    print(df_stats)\n","\n","    #---------------------------------------------------\n","\n","    # Use plot styling from seaborn.\n","    sns.set(style='darkgrid')\n","\n","    # Increase the plot size and font size.\n","    sns.set(font_scale=1.5)\n","    plt.rcParams[\"figure.figsize\"] = (12,6)\n","\n","    # Plot the learning curve.\n","    plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n","    plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n","\n","    # Label the plot.\n","    plt.title(\"Training & Validation Loss\")\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Loss\")\n","    plt.legend()\n","    plt.xticks([1, 2, 3, 4])\n","\n","    plt.show()\n","\n","    plt.figure(figsize=(15,8))\n","    plt.title(\"Training loss\")\n","    plt.xlabel(\"Batch\")\n","    plt.ylabel(\"Loss\")\n","    plt.plot(train_loss_set)\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ct2S5J0lPhoB"},"outputs":[],"source":["# converting list of class weights to a tensor\n","weights= torch.tensor(class_weights_list[best_class_weight],dtype=torch.float)\n","\n","# push to GPU\n","weights = weights.to(device)\n","\n","# define the loss function\n","cross_entropy  = nn.CrossEntropyLoss(weight=weights) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WqAyB5bePiGs"},"outputs":[],"source":["train = pd.DataFrame({'sentence':sentences, 'target':train['target']})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M5Nzv5yxPnnE"},"outputs":[],"source":["# Load BertForSequenceClassification, the pretrained BERT model with a single \n","# linear classification layer on top. \n","model = BertForSequenceClassification.from_pretrained(\n","    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n","    num_labels = 4, # The number of output labels--4 for multi-class classification. \n","    output_attentions = False, # Whether the model returns attentions weights.\n","    output_hidden_states = False, # Whether the model returns all hidden-states.\n",")\n","\n","for param in model.bert.parameters():\n","    param.requires_grad = False\n","\n","# Tell pytorch to run this model on the GPU.\n","model.cuda()\n","\n","# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n","# I believe the 'W' stands for 'Weight Decay fix\"\n","optimizer = AdamW(model.parameters(),\n","                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n","                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n","                )\n","\n","#------------------------------------------------------\n","\n","# Get the lists of sentences and their labels.\n","train_sentences = train.sentence.values\n","train_labels = train.target.values \n","\n","# Tokenize all of the sentences and map the tokens to thier word IDs.\n","train_input_ids = []\n","train_attention_masks = []\n","\n","    # For every sentence...\n","for sent in train_sentences:\n","    # `encode_plus` will:\n","    #   (1) Tokenize the sentence.\n","    #   (2) Prepend the `[CLS]` token to the start.\n","    #   (3) Append the `[SEP]` token to the end.\n","    #   (4) Map tokens to their IDs.\n","    #   (5) Pad or truncate the sentence to `max_length`\n","    #   (6) Create attention masks for [PAD] tokens.\n","    encoded_dict = tokenizer.encode_plus(\n","                        sent,                      # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                        max_length = 50,           # Pad & truncate all sentences.\n","                        pad_to_max_length = True,\n","                        return_attention_mask = True,   # Construct attn. masks.\n","                        return_tensors = 'pt',     # Return pytorch tensors.\n","                  )\n","    \n","    # Add the encoded sentence to the list.    \n","    train_input_ids.append(encoded_dict['input_ids'])\n","    \n","    # And its attention mask (simply differentiates padding from non-padding).\n","    train_attention_masks.append(encoded_dict['attention_mask'])\n","\n","# Convert the lists into tensors.\n","train_input_ids = torch.cat(train_input_ids, dim=0)\n","train_attention_masks = torch.cat(train_attention_masks, dim=0)\n","train_labels = torch.tensor(train_labels)\n","\n","#---------------------------------------------------\n","\n","# Combine the training inputs into a TensorDataset.\n","train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n","\n","#---------------------------------------------------\n","\n","# The DataLoader needs to know our batch size for training, so we specify it \n","# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n","# size of 16 or 32.\n","batch_size = 32\n","\n","# Create the DataLoaders for our training and validation sets.\n","# We'll take training samples in random order. \n","train_dataloader = DataLoader(\n","            train_dataset,  # The training samples.\n","            sampler = RandomSampler(train_dataset), # Select batches randomly\n","            batch_size = batch_size # Trains with this batch size.\n","        )\n","\n","#---------------------------------------------------\n","\n","# Number of training epochs. The BERT authors recommend between 2 and 4. \n","# We chose to run for 4, but we'll see later that this may be over-fitting the\n","# training data.\n","epochs = 2\n","\n","# Total number of training steps is [number of batches] x [number of epochs]. \n","# (Note that this is not the same as the number of training samples).\n","total_steps = len(train_dataloader) * epochs\n","\n","# Create the learning rate scheduler.\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 0, # Default value in run_glue.py\n","                                            num_training_steps = total_steps)\n","\n","#---------------------------------------------------\n","\n","# Set the seed value all over the place to make this reproducible.\n","seed_val = 42\n","train_loss_set = []\n","\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","# We'll store a number of quantities such as training and validation loss, \n","# validation accuracy, and timings.\n","training_stats = []\n","\n","# Measure the total training time for the whole run.\n","total_t0 = time.time()\n","\n","# For each epoch...\n","for epoch_i in range(0, epochs):\n","    \n","    # ========================================\n","    #               Training\n","    # ========================================\n","    \n","    # Perform one full pass over the training set.\n","\n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","\n","    # Measure how long the training epoch takes.\n","    t0 = time.time()\n","\n","    # Reset the total loss for this epoch.\n","    total_train_loss = 0\n","\n","    # Put the model into training mode. Don't be mislead--the call to \n","    # `train` just changes the *mode*, it doesn't *perform* the training.\n","    # `dropout` and `batchnorm` layers behave differently during training\n","    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n","    model.train()\n","\n","    # For each batch of training data...\n","    for step, batch in enumerate(train_dataloader):\n","\n","        # Progress update every 40 batches.\n","        if step % 40 == 0 and not step == 0:\n","            # Calculate elapsed time in minutes.\n","            elapsed = format_time(time.time() - t0)\n","            \n","            # Report progress.\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","        # Unpack this training batch from our dataloader. \n","        #\n","        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n","        # `to` method.\n","        #\n","        # `batch` contains three pytorch tensors:\n","        #   [0]: input ids \n","        #   [1]: attention masks\n","        #   [2]: labels \n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","\n","        # Always clear any previously calculated gradients before performing a\n","        # backward pass. PyTorch doesn't do this automatically because \n","        # accumulating the gradients is \"convenient while training RNNs\". \n","        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n","        optimizer.zero_grad()        \n","\n","        # Perform a forward pass (evaluate the model on this training batch).\n","        # The documentation for this `model` function is here: \n","        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","        # It returns different numbers of parameters depending on what arguments\n","        # arge given and what flags are set. For our useage here, it returns\n","        # the loss (because we provided labels) and the \"logits\"--the model\n","        # outputs prior to activation.\n","        outputs = model(b_input_ids, \n","                            token_type_ids=None, \n","                            attention_mask=b_input_mask, \n","                            labels=b_labels)\n","        \n","        loss = cross_entropy(outputs.logits, b_labels)\n","\n","        train_loss_set.append(loss.item()) \n","\n","        # Accumulate the training loss over all of the batches so that we can\n","        # calculate the average loss at the end. `loss` is a Tensor containing a\n","        # single value; the `.item()` function just returns the Python value \n","        # from the tensor.\n","\n","        total_train_loss += loss.item()\n","\n","        # Perform a backward pass to calculate the gradients.\n","        loss.backward()\n","\n","        # Clip the norm of the gradients to 1.0.\n","        # This is to help prevent the \"exploding gradients\" problem.\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        # Update parameters and take a step using the computed gradient.\n","        # The optimizer dictates the \"update rule\"--how the parameters are\n","        # modified based on their gradients, the learning rate, etc.\n","        optimizer.step()\n","\n","        # Update the learning rate.\n","        scheduler.step()\n","\n","    # Calculate the average loss over all of the batches.\n","    avg_train_loss = total_train_loss / len(train_dataloader)            \n","    \n","    # Measure how long this epoch took.\n","    training_time = format_time(time.time() - t0)\n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epcoh took: {:}\".format(training_time))\n","\n","\n","print(\"\")\n","print(\"Training complete!\")"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"BERT_ContinuousFeature.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
