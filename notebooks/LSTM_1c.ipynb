{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"GGMyY5PtvRaC"},"outputs":[],"source":["from numpy import array\n","from keras.preprocessing.text import one_hot\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers.core import Activation, Dropout, Dense, SpatialDropout1D\n","from keras.layers import Flatten, LSTM\n","from keras.layers import GlobalMaxPooling1D\n","from sklearn.model_selection import cross_val_score\n","from sklearn.model_selection import StratifiedKFold\n","from keras.models import Model\n","from keras.layers.embeddings import Embedding\n","from keras.preprocessing.text import Tokenizer\n","from keras.layers import Input\n","from keras.layers.merge import Concatenate\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","\n","from sklearn import preprocessing\n","\n","import pandas as pd\n","import numpy as np\n","import re\n","\n","from numpy import array\n","from numpy import asarray\n","from numpy import zeros\n","\n","import nltk\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","\n","from keras.utils.np_utils import to_categorical\n","\n","from keras.utils.vis_utils import plot_model\n","\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oGUH2crL4wkM"},"outputs":[],"source":["STOPWORDS = set(stopwords.words('english'))\n","REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4F_1W-MAuFBS"},"outputs":[],"source":["def preprocess_text(sen):\n","\n","    stemmer = WordNetLemmatizer()\n","\n","   # Remove all the special characters\n","    document = re.sub(r'\\W', ' ', str(sen))\n","    \n","    # remove all single characters\n","    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n","    \n","    # Remove single characters from the start\n","    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n","\n","    document = REPLACE_BY_SPACE_RE.sub(' ', document)\n","    \n","    # Substituting multiple spaces with single space\n","    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n","    \n","    # Converting to Lowercase\n","    document = document.lower()\n","  \n","    document = document.split()\n","\n","    document = ' '.join(word for word in document if word not in STOPWORDS) # remove stopwors from text\n","\n","    # Lemmatization\n","  \n","    document = document.split()\n","\n","    document = [stemmer.lemmatize(word) for word in document]\n","    document = ' '.join(document) \n","    \n","    return document"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JsVgTGgCICEX"},"outputs":[],"source":["def getModel(length_long_sentence, embedding_matrix):\n","  deep_inputs = Input(shape=(length_long_sentence,))\n","  embedding_layer = Embedding(vocab_size, 50, weights=[embedding_matrix], trainable=False)(deep_inputs)\n","  LSTM_Layer_1 = LSTM(70, activation='tanh')(embedding_layer)\n","  dense_layer_1 = Dense(35, activation='tanh')(LSTM_Layer_1)\n","  dense_layer_2 = Dense(4, activation='softmax')(dense_layer_1)\n","  \n","  return Model(inputs=deep_inputs, outputs=dense_layer_2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SVqy0xiTF96u"},"outputs":[],"source":["def weighted_categorical_crossentropy(weights):\n","    \"\"\"\n","    A weighted version of keras.objectives.categorical_crossentropy\n","\n","    Variables:\n","        weights: numpy array of shape (C,) where C is the number of classes\n","\n","    Usage:\n","        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n","        loss = weighted_categorical_crossentropy(weights)\n","        model.compile(loss=loss,optimizer='adam')\n","    \"\"\"\n","\n","    weights = K.variable(weights)\n","\n","    def loss(y_true, y_pred):\n","        # scale predictions so that the class probas of each sample sum to 1\n","        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n","        # clip to prevent NaN's and Inf's\n","        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n","        # calc\n","        loss = y_true * K.log(y_pred) * weights\n","        loss = -K.sum(loss, -1)\n","        return loss\n","\n","    return loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fDb9WSugGYdX"},"outputs":[],"source":["train_path = 'Training_set.csv'\n","\n","col_names = ['triggerTitle','triggerChannelTitle','actionChannelTitle','actionTitle','title', 'desc', 'target']\n","train_df = pd.read_csv(train_path,skiprows=1,sep=';',names=col_names,encoding = \"ISO-8859-1\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2ox-BvJKMxdH"},"outputs":[],"source":["#Pre-processing\n","\n","X_train = []\n","for i in range(0,len(train_df)):\n","  X_train.append(preprocess_text(train_df.iloc[i][4] + \". \" + train_df.iloc[i][5] + \".\"))\n","\n","y = train_df['target']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kfJpvDktsGxA"},"outputs":[],"source":["test_path = 'Test_set.csv'\n","\n","col_names = ['triggerTitle','triggerChannelTitle','actionChannelTitle','actionTitle','title', 'desc', 'target']\n","test_df = pd.read_csv(test_path,skiprows=1,sep=';',names=col_names,encoding = \"ISO-8859-1\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZRROtEtwsuZH"},"outputs":[],"source":["#Pre-processing\n","\n","X_test = []\n","for i in range(0,len(test_df)):\n","  X_test.append(preprocess_text(test_df.iloc[i][4] + \". \" + test_df.iloc[i][5] + \". \"))\n","\n","y_test = test_df['target']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fWPOLnH0Nc7Y"},"outputs":[],"source":["y_train = to_categorical(y_train)\n","y_test = to_categorical(y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WgBQADdGN6ae"},"outputs":[],"source":["tokenizer = Tokenizer(lower=True, num_words=5000, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~')\n","tokenizer.fit_on_texts(X_train)\n","\n","word_index = tokenizer.word_index\n","print('Found %s unique tokens.' % len(word_index))\n","\n","X_train = tokenizer.texts_to_sequences(X_train)\n","X_test = tokenizer.texts_to_sequences(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FIHqTfDPOnxs"},"outputs":[],"source":["vocab_size = len(tokenizer.word_index) + 1\n","\n","length_long_sentence = 50\n","\n","X_train = pad_sequences(X_train,  maxlen=length_long_sentence, padding='post')\n","X_test = pad_sequences(X_test,  maxlen=length_long_sentence,padding='post')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"32Id4-Q3OsEW"},"outputs":[],"source":["embeddings_dictionary = dict()\n","glove_file = open('Glove/glove.6B.50d.txt', encoding=\"utf8\")\n","\n","for line in glove_file:\n","    records = line.split()\n","    word = records[0]\n","    vector_dimensions = asarray(records[1:], dtype='float32')\n","    embeddings_dictionary [word] = vector_dimensions\n","\n","glove_file.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3vlQOADrO6iD"},"outputs":[],"source":["embedding_matrix = zeros((vocab_size, 50))\n","for word, index in tokenizer.word_index.items():\n","    embedding_vector = embeddings_dictionary.get(word)\n","    if embedding_vector is not None:\n","        embedding_matrix[index] = embedding_vector"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v2d9YYNSIr8R"},"outputs":[],"source":["# STRATIFIES K-FOLD CROSS VALIDATION { 4-fold }\n","\n","splits = 4\n","\n","# Create StratifiedKFold object.\n","skf = StratifiedKFold(n_splits=splits, shuffle=True, random_state=1)\n","\n","class_weights_list = []\n","\n","for train_index, test_index in skf.split(X_train, y_train):\n","    x_train_fold, x_test_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n","    y_train_fold, y_test_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n","\n","    #compute the class weights\n","    class_weights = compute_class_weight('balanced', np.unique(y_train_fold),y = np.ravel(y_train_fold))\n","\n","    class_weights_list.append(class_weights)\n","\n","    ncce = weighted_categorical_crossentropy(weights=np.array(class_weights))\n","\n","    model = getModel(length_long_sentence, embedding_matrix)\n","\n","    model.compile(loss=ncce, optimizer='adam', metrics=['acc'])\n","\n","    model.fit(x_train_fold, y_train_fold, batch_size=10, epochs=24, verbose=1)\n","\n","    score = model.evaluate(x_test_fold, y_test_fold, verbose=0)\n","\n","    print(\"Accuracy Validation: %.2f%%\" % (score[1]*100))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"87XbyMirIkey"},"outputs":[],"source":["ncce = weighted_categorical_crossentropy(weights=np.array(class_weights_list[best_class_weight]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eJbCIn5EPbaV"},"outputs":[],"source":["model = getModel(length_long_sentence, embedding_matrix)\n","\n","model.compile(loss=ncce, optimizer='adam', metrics=['acc'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y17c7MvSPmln"},"outputs":[],"source":["history = model.fit(X_train, y_train, batch_size=10, epochs=24, verbose=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TBC_fVliVqG3"},"outputs":[],"source":["score = model.evaluate(X_test, y_test, verbose=0)\n","\n","print(\"Accuracy Test: %.2f%%\" % (score[1]*100))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eeitDpx2y_KO"},"outputs":[],"source":["from sklearn.metrics import classification_report\n","from sklearn.metrics import confusion_matrix\n","\n","y_pred = model.predict(X_test)\n","\n","y_pred_clean = np.zeros_like(y_pred)\n","for idx, i in enumerate(np.argmax(y_pred,axis=1)):\n","    y_pred_clean[idx][i] = 1\n","\n","print(classification_report(y_test, y_pred_clean))"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"LSTM_ContinuousFeature.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
